{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d231014",
   "metadata": {},
   "source": [
    "# Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce0be6",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6b1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35b94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c541b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name'].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.3,\n",
    "                                                   random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfff518",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c25db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n TRAINING STATS:')\n",
    "print('classification accuracy:', metrics.accuracy_score(y_pred, y_train))\n",
    "print('confusion matrix: \\n', metrics.confusion_matrix, y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec073d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n TESTING STATS:')\n",
    "print('classification accuracy:', metrics.accuracy_score(y_pred, y_test))\n",
    "print('confusion matrix: \\n', metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c07ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, predictions)).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747b6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f7cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e19053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test split\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,\n",
    "                                                                    test_size=0.20)\n",
    "\n",
    "# create a 10-fold cross-validation set\n",
    "kf = cross_validation.KFold(n=y.shape[0],\n",
    "                           n_folds=10,\n",
    "                           shuffle=True,\n",
    "                           random_state=0)\n",
    "\n",
    "# search for parameter along the following:\n",
    "C = np.arange(2, 20,)\n",
    "acc = np.zeros((10,18))\n",
    "i = 0\n",
    "for train_index, val_index in kf:\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    j = 0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(\n",
    "            min_smaples_leaf=1, \n",
    "            max_depth=c)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_val)\n",
    "        acc[i][j] = metrics.accuracy_score(y_pred, y_val)\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "print('Mean accuracy:', np.mean(acc, axis=0))\n",
    "print('Selected model index:', np.argmax(np.mean(acc, axis=0)))\n",
    "    \n",
    "    \n",
    "# source intro to data science (Igual & SeguÃ­) p. 84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758e43c",
   "metadata": {},
   "source": [
    "#### Extending to 2+ Features\n",
    "\n",
    "From a theory standpoint, adding more features to our logistic regression model simply means learning a new coefficient in the linear combination for each feature when training -- from another perspective, it means finding a higher dimensional hyperplane that separates the data as well as possible.\n",
    "\n",
    "Practically, with **sklearn logistic regression we should make sure to scale our features** prior to fitting, since regularzation is used by default (that or set C very high). It can be as simple as the below-- \n",
    "\n",
    "source metis lecture on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "\n",
    "X_train = train_df[['elevation', 'price_per_sqft']]\n",
    "X_train_scaled = std_scale.fit_transform(X_train)\n",
    "\n",
    "lm3 = LogisticRegression()\n",
    "lm3.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_predict = lm3.predict(X_train_scaled) \n",
    "lm3.score(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170977ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C = 0.95)\n",
    "logit.fit(X_train, label_train)\n",
    "print(\"The score for logistic regression is\")\n",
    "print(\"Training: {:6.2f}%\".format(100*logit.score(X_train, label_train)))\n",
    "print(\"Test set: {:6.2f}%\".format(100*logit.score(X_test, label_test)))\n",
    "\n",
    "# source metis classification_error_metrics_solutions_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for logistic regression\n",
    "logit_confusion = confusion_matrix(label_test, logit.predict(X_test))\n",
    "plt.figure(dpi=150)\n",
    "sns.heatmap(logit_confusion, cmap=plt.cm.Blues, annot=True, square=True,\n",
    "           xticklabels=iris_dataset['target_names'],\n",
    "           yticklabels=iris_dataset['target_names'])\n",
    "\n",
    "plt.xlabel('Predicted species')\n",
    "plt.ylabel('Actual species')\n",
    "plt.title('Logistic regression confusion matrix');\n",
    "\n",
    "plt.savefig(\"confusion_matrix_logit_iris\")\n",
    "\n",
    "# source metis classification_error_metrics_solutions_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(model, threshold=0.5):\n",
    "    # Predict class 1 if probability of being in class 1 is greater than threshold\n",
    "    # (model.predict(X_test) does this automatically with a threshold of 0.5)\n",
    "    y_predict = (model.predict_proba(X_test)[:, 1] >= threshold)\n",
    "    fraud_confusion = confusion_matrix(y_test, y_predict)\n",
    "    plt.figure(dpi=80)\n",
    "    sns.heatmap(fraud_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',\n",
    "           xticklabels=['legit', 'fraud'],\n",
    "           yticklabels=['legit', 'fraud']);\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('actual')\n",
    "    \n",
    "make_confusion_matrix(lm)\n",
    "\n",
    "\n",
    "# source metis classification_error_metrics_solutions_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how our confusion matrix changes with changes to the cutoff! \n",
    "from ipywidgets import interactive, FloatSlider\n",
    "interactive(lambda threshold: make_confusion_matrix(lm, threshold), threshold=(0.0,1.0,0.02))\n",
    "\n",
    "# source metis classification_error_metrics_solutions_revised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aac661",
   "metadata": {},
   "source": [
    "- **Precision:** The fraction of postive predictions you made that were correct. \n",
    "  High precision means that if your model predicted a positive case, you believe it with high confidence. It doesn't tell us how many postive cases we missed (i.e. it doesn't tell us how sure we are about the cases we predicted were negative).\n",
    "- **Recall**: The fraction of positive cases you predicted correctly.\n",
    "  High recall means that you are confident that you didn't miss any positive cases. \n",
    " \n",
    "\n",
    "## Big takeaways:\n",
    "\n",
    "* Using the **same** logistic regression model, we can change the threshold to bias toward more precision (making positives from test more relevant) or recall (increasing the fraction of postives found).\n",
    "* Precision goes down as you decrease the threshold, while recall goes up. This is called the _precision-recall tradeoff_.\n",
    "* Which is worse, low recall or low precision? Depends on the cost of making the different types of error.\n",
    "* We just need the final predictions of the model to calculate precision and recall. We can get sklearn.metrics to calculate them for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default threshold of 0.5, which is what vanilla predict does\n",
    "y_predict = lm.predict(X_test)\n",
    "print(\"Default threshold:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_test, y_predict), \n",
    "                                                     recall_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the new threshold of 0.06\n",
    "y_predict = (lm.predict_proba(X_test)[:,1] > 0.06)\n",
    "print(\"Threshold of 0.06:\")\n",
    "print(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_test, y_predict), \n",
    "                                                     recall_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the probabilities to make a curve showing us how recall \n",
    "# and thresholds trade off \n",
    "\n",
    "precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_test, lm.predict_proba(X_test)[:,1] )\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.plot(threshold_curve, precision_curve[1:],label='precision')\n",
    "plt.plot(threshold_curve, recall_curve[1:], label='recall')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Threshold (above this probability, label as fraud)');\n",
    "plt.title('Precision and Recall Curves');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edadfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "plt.plot(recall_curve[1:], precision_curve[1:],label='precision')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cec1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can just ask sklearn\n",
    "y_predict = lm.predict(X_test)\n",
    "f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e02d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the threshold of 0.06?\n",
    "y_predict = (lm.predict_proba(X_test)[:, 1] > 0.06)\n",
    "f1_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f67001",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The ROC curve\n",
    "\n",
    "# We've already seen that we don't have to accept a 50% threshold cutoff. \n",
    "# As we've seen, we can plot our models with different thresholds on the same chart and get a ROC curve. \n",
    "# This curve plots the *true positive rate* on the y axis, and the *false positive rate* on the x axis.\n",
    "\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP/P = True positive rate\n",
    "# false positive rate = FP / true negatives = FP / (FP + TN) \n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lm.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr,lw=2)\n",
    "plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve for fraud problem');\n",
    "print(\"ROC AUC score = \", roc_auc_score(y_test, lm.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ROC curve (receiver operating characteristic curve) is a graph showing the performance \n",
    "# of a classification model at all classification thresholds. This curve plots two parameters: \n",
    "# True Positive Rate. False Positive Rate.\n",
    "\n",
    "#  source https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (unsupervised)",
   "language": "python",
   "name": "unsupervised"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
